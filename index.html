<!--++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 * Created by: Peng Wang
 *
 * Borrowed from the html from Mu Li < https://www.cs.cmu.edu/~muli/ >
 * website is host on: freehosting
 *+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-->
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
"http://www.w3.org/TR/html4/strict.dtd">
<html dir="LTR" lang="en">

<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Peng Wang's Homepage</title>
<script type="text/javascript"></script>
<link rel="stylesheet" media="all" type="text/css"
      href="style.css">
<meta name="description" content="Peng Wang's homepage">
<link rel="shortcut icon" href="./img/icon.png">
<!--[if lte IE 6]><link rel="stylesheet" media="screen" type="text/css"
href="ie6.css"><![endif]-->
<script type="text/javascript">
  var _gaq = _gaq || []; _gaq.push(['_setAccount', 'UA-20567385-1']);
  _gaq.push(['_trackPageview']);

  (function() { var ga = document.createElement('script'); 
    ga.type = 'text/javascript'; ga.async = true; ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') +
    '.google-analytics.com/ga.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })();
</script>
<!-- <script type="text/javascript" src="http://s46.sitemeter.com/js/counter.js?site=s46mulibcmi"></script> -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
</head>

<body>

<ol id="nav">
    <li><a class="selected" href="./index.html" title="Home">Home</a></li>
    <li><a href="#res" title="Research Interests">Interest</a></li>
    <li><a href="#news" title="News">News</a></li>
    <li><a href="#pub" title="Papers">Publication</a></li>
    <li><a href="#prod" title="Products">Products</a></li>
    <li><a href="#tea" title="Teaching">Teaching</a></li>
    <li><a href="#data" title="Datasets">Dataset</a></li>
    <li><a href="#soft" title="Software">Software</a></li>
    <li> <a href="./blog/"><span class="fa fa-pencil"></span> Blog</a></li>
</ol>

<div id="wrapper">
<div id="name_chn"></div>
<div id="content">
  <a href="./img/peng.jpg"><img id="head_photo" src="./img/peng.jpg"/></a>

<div id="motto_logo">
  <img src="img/motto.png" height="40px" />
</div>
<div id="person_intro">
  <h1 id="name"><a href="peng-wang.com" title="Peng Wang's homepage"> Peng Wang </a></h1> Staff Research Scientist at Bytedance, USA<br>
   <br>
   I obtained PhD from <a href="http://www.ucla.edu/"> University of California, Los Angeles </a> under the supervision of 
   <a href="http://www.cs.jhu.edu/~ayuille/"> Prof. Alan Yuille</a>.  
   I received the M.S degree of Machine Intelligence at 2013 under the supervision of 
   <a href="www.cis.pku.edu.cn/faculty/vision/zeng/"> Prof. Gang Zeng</a> and 
   <a href="www.cis.pku.edu.cn/vision/Visual&Robot/people/zha/"> Prof. Hongbin Zha</a>,  
   and B.S. degree of Artificial Intelligence in <a href="http://english.pku.edu.cn/"> Peking University </a> in 2010.  
   During my study, 
   I was interned in MSRA with 
   <a href="https://www.microsoft.com/en-us/research/people/jingdw/"> Dr. Jingdong Wang</a>, 
   Adobe Research with 
   <a href="http://users.eecs.northwestern.edu/~xsh835/"> Dr. Xiaohui Shen</a>, 
   <a href="https://research.adobe.com/person/zhe-lin/"> Dr. Zhe Lin</a>, 
   <a href="https://research.adobe.com/person/scott-cohen/"> Dr. Scott Cohen</a> and 
   <a href="https://research.adobe.com/person/brian-price/"> Dr. Brian Price</a>, 
   and Google Research with 
   <a href="https://ai.google/research/people/KevinMurphy"> Prof. Kevin Murphy</a> and 
   <a href="https://ai.google/research/people/105009"> Dr. Sergio Guadarrama.<br> </a>

   <br> We are currently <font color="red">hiring</font> self-motivated research scientist/ research engineer / interns especially on SLAM, deep semantic/instance segmentation, depth estimation and efficient neural architecture training/search. Please send me an email if you are interested. <br>
   <br> We apply many from our research to products such as Intelligent Album, Douyin/TikTok AR effects (check products) with dense scene understanding etc. <br>

<!-- <p> <i class="fa fa-envelope"></i> 7713 GHC, 5000 Forbes Ave. Pittsburgh, PA 15213</p> -->
<p> <i class="fa fa-inbox"></i> <a>jerryking234 [at] gmail [dot] com</a>
  <br>
  <i class="fa fa-align-left"></i> <a href="https://scholar.google.com/citations?user=Svk4ntYAAAAJ&hl=en">Google Scholar</a>
<br>
 <i class="fa fa-github"></i> <a href="https://github.com/pengwangucla">Github</a>
</p>
</div>

<a name="res"></a>
<h1>Research Interests</h1>

<div id="research_interests">
    My research interest is focusing on Computer Vision, Machine Learning and Image Processing.  
    I am currently exploiting the possibility of learning 3D representations from videos, learning neural architectures, and mining informative instances from datasets.
    ​
    I upload some <a href="https://drive.google.com/file/d/1VJBN4sE5GNTMEMFbS_tJFrXCAb51t3IZ/view?usp=sharing"> research thoughts </a> about how to learn visual system in an unsupervised manner.
</div>

<div id="clear"></div>
<a name="news"></a>
<h1>News</h1>

<!-- News -->
<div class="news" style="overflow:auto; height:110px; padding-top: 10px;">
<ul>
    <li>[Jun, 2022] one papers to <a href=""> CVPR 2022 </a>. </li>
    <li>[Feb, 2022] three product features delivered to <a href="#prod"> Douyin, Jianying </a>.</li>
    <li>[Feb, 2021] one papers to <a href=""> CVPR 2021 </a>, one paper to <a href=""> ACCV 2020 </a>.</li>
    <li>[Feb, 2020] one papers to <a href=""> ICRA 2020 </a>, one paper to <a href=""> CVPR 2020 </a>.</li>
    <li>[Jul, 2019] Tree papers to <a href=""> TPAMI 2019 </a>.</li>
    <li>[Jul, 2019] Two papers to <a href=""> AAAI 2020 </a>.</li>
    <li>[Jul, 2019] Tree papers to <a href=""> TPAMI 2019 </a> and one to <a href="">BMVC 2019</a>.</li>
    <li>[Feb, 2019] Two papers are accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.</li>
    <li>[Jun, 2018] We are holding an <a href="http://wad.ai"> Autonomous Driving Workshop at ECCV 2018/CVPR 2019</a>.</li>
    <li>[May, 2018] One ECCV 2018 (No.1 at KITTI stereo) & BMVC paper accepted.</li>
    <li>[Feb, 2018] Five paper to <a href="http://cvpr2018.thecvf.com/">CVPR 2018</a>.</li>
    <li>[Jul, 2017] One paper to <a href="http://aaai2018.thecvf.com/">AAAI 2018</a>.</li>
    <li>[Jul, 2017] One paper to <a href="http://cvpr2017.thecvf.com/">CVPR 2017 </a>, dataset of PASCAL human part and keypoints is released </a>.</li>
  </ul>
</div>

<!-- Publications -->

<div id="clear"></div>

<a name="pub"></a>

<div id="publication">
<h1>Papers</h1>
<h2>2022</h2>
<ul>
  <li>
  <a href="https://arxiv.org/abs/2204.05547">
    DistPro: Searching A Fast Knowledge Distillation Process via Meta Optimization
  </a>
  <br>
    Xueqing Deng, Dawei Sun, Shawn Newsam, <b>Peng Wang</b>
  <br>
  <a>In Submission</a>, 2022. 
</ul>
<ul>
  <li>
  <a href="https://arxiv.org/abs/2205.15401">
    VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis
  </a>
  <br>
    Angtian Wang, <b>Peng Wang</b>, Jian Sun, Adam Kortylewski, Alan Yuille
  <br>
  <a>In Submission</a>, 2022. 
  <a href="https://github.com/Angtian/VoGE">
    <i class="fa fa-code"></i>code</a>
</li>
</ul>
<ul>
  <li>
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_NightLab_A_Dual-Level_Architecture_With_Hardness_Detection_for_Segmentation_at_CVPR_2022_paper.pdf">
    NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night
  </a>
  <br>
    Xueqing Deng, <b>Peng Wang</b>, Xiaochen Lian, Shawn Newsam
  <br>
  <a>CVPR</a>, 2022. 
  <a href="https://github.com/xdeng7/NightLab">
    <i class="fa fa-code"></i>code</a>
</li>
</ul>

<h2>2021</h2>
<ul>
  <li>
  <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Ding_HR-NAS_Searching_Efficient_High-Resolution_Neural_Architectures_With_Lightweight_Transformers_CVPR_2021_paper.html">
    HR-NAS: Searching Efficient High-Resolution Neural Architectureswith Lightweight Transformers
  </a>
  <br>
    Mingyu Ding, Xiaochen Lian, Linjie Yang, <b>Peng Wang</b>, Xiaojie Jin, Zhiwu Lu, Ping Luo
  <br>
  <a>CVPR</a>, 2021. (<span class="bold-red">Oral</span>) 
  <a href="https://github.com/dingmyu/HR-NAS">
    <i class="fa fa-code"></i>code</a>
</li>
</ul>

<h2>2020</h2>
<ul>
  <li>
  <a href="https://arxiv.org/abs/1911.05377">
      3D Part Guided Image Editing for Fine-grained Object Understanding
  </a>
  <br>
    Zongdai Liu, Feixiang Lu, <b>Peng Wang</b>, Hui Miao, Liangjun Zhang,  Ruigang Yang, Bin Zhou
  <br>
  <a>CVPR</a>, 2020. 
</li>
<li>
  <a href="https://arxiv.org/abs/1911.05377">
      Omnidirectional Depth Extension Networks
  </a>
  <br>
    Xinjing Chen, <b>Peng Wang­<sup>+</sup></b>, Yanqi Zhou, Chenye Guan, Ruigang Yang
  <br>
  <a>ICRA</a>, 2020. (<sup>+</sup>Corresponding author))
</li>
<li>
  <a href="https://arxiv.org/abs/1911.05377">
      CSPN++: Learning Context and Resource Aware Convolutional Spatial Propagation Networks for Depth Completion
  </a>
  <br>
    Xinjing Chen, <b>Peng Wang­<sup>+</sup></b>, Chenye Guan, Ruigang Yang
  <br>
  <a>AAAI</a>, 2020. (<sup>+</sup>Corresponding author))
</li>
<li>
  <a>
      AutoRemover: Automatic Object Removal for Autonomous Driving Videos
  </a>
  <br>
    Rong Zhang, Wei Li<sup>+</sup>, <b>Peng Wang­<sup>+</sup></b>, Chenye Guan, Jin Fang， Yuhang Song， Jinhui Yu， Baoquan Chen， Weiwei Xu<sup>+</sup>， Ruigang Yang
  <br>
  <a>AAAI</a>, 2020. (<sup>+</sup>Corresponding authors)
</li>
<li>
  <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Liao_Speech2Video_Synthesis_with_3D_Skeleton_Regularization_and_Expressive_Body_Poses_ACCV_2020_paper.html">
    Speech2Video Synthesis with 3D Skeleton Regularization and Expressive Body Poses
  </a>
  <br>
    Miao Liao, Sibo Zhang, <b>Peng Wang</b>, Hao Zhu, Xinxin Zuo, Ruigang Yang
  <br>
  <a>ACCV</a>, 2020. 
</li>
</ul>

<h2>2019</h2>
<ul>
  <li>
    <a href="https://arxiv.org/abs/1810.02695">
      Learning Depth with Convolutional Spatial Propagation Network
    </a>
    <br>
      Xinjing Chen, <b>Peng Wang­<sup>+</sup></b>, Ruigang Yang
    <br>
    <a href="https://mc.manuscriptcentral.com/tpami-cs">TPAMI</a>, 2019. (<sup>+</sup>Corresponding author))
  </li>
  <li>
    <a href="https://arxiv.org/abs/1810.06125">
      Every Pixel Counts++: Joint Learning of Geometry and Motion with 3D Holistic Understanding
    </a>
    <br>Chenxu Luo*, Zhenheng Yang*, <b>Peng Wang*<sup>+</sup></b>, Yang Wang, Wei Xu, Ram Nevatia, Alan Yuille
    <br>
    <a href="https://mc.manuscriptcentral.com/tpami-cs">TPAMI</a>, 2019. (* Equal contribution, <sup>+</sup>Corresponding author)
  </li>
  <li>
    <a href="https://arxiv.org/abs/1803.06184">
      The ApolloScape Open Dataset for Autonomous Driving and its Applicationg
    </a>
    <br>Xinyu Huang*, <b>Peng Wang*</b>, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, Ruigang Yang
    <br>
    <a href="">TPAMI</a>, 2019. (*Equal contribution)
  </li>
  <li>
    <a href="">
      EPNAS: Efficient Progressive Neural Architecture Search    
    </a>
    <br>Yanqi Zhou, <b>Peng Wang</b>,  Sercan Arik, Haonan Yu, Syed Zawad, Feng Yan, Greg Diamos
    <br>
    <a href="">BMVC</a>, 2019.
  </li>
  <li>
    <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_UnOS_Unified_Unsupervised_Optical-Flow_and_Stereo-Depth_Estimation_by_Watching_Videos_CVPR_2019_paper.pdf">
      UnOS: Unified Unsupervised Optical-flow and Stereo-depth Estimation by Watching Videos
    </a>
    <br>Yang Wang, <b>Peng Wang</b>, Zhenheng Yang,  Chenxu Luo, Yi Yang, Wei Xu
    <br>
    <a href="">CVPR</a>, 2019.
    <a href="https://github.com/baidu-research/UnDepthflow">
      <i class="fa fa-code"></i>code</a>
  </li>
  <li>
    <a href="https://arxiv.org/abs/1811.12222">
      ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving
    </a>
    <br>Xibin Song, <b>Peng Wang</b>, Dingfu Zhou, Rui Zhu, Chenye Guan, Yunchao Dai, Hao Su, Hongdong Li, Ruigang Yang
    <br>
    <a href="">CVPR</a>, 2019.
    <a href="https://github.com/ApolloScapeAuto/dataset-api/tree/master/car_instance">
      <i class="fa fa-database"></i>data</a>
  </li>
</ul>

<h2>2018</h2>
<ul>
  <li>
    <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Yang_Every_Pixel_Counts_Unsupervised_Geometry_Learning_with_Holistic_3D_Motion_ECCVW_2018_paper.pdf">
      Every Pixel Counts: Unsupervised Geometry Learning with Holistic 3D Motion Understanding
    </a>
    <br>Zhenheng Yang, <b>Peng Wang</b>, Yang Wang, Wei Xu, Ramakant Nevatia<br>
    <a href="http://apolloscape.auto/ECCV/index.html">ECCV (Workshop of VNAD )</a>, 2018
    <a href="">
    <i class="fa fa-code"></i>code (to be update)</a>
  </li>
<!--     <red> We jointly learning edge, depth, normal, moving object mask in a self-supervised manner using video only. </red>
 -->  
  <li>
    <a href="https://arxiv.org/abs/1808.00150">
       Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network
    </a>
    <br>Xinjing Chen*, <b>Peng Wang*</b>, Ruigang Yang<br>
    <a href="">ECCV</a>, 2018 (* Equal contribution)
    <a href="https://github.com/XinJCheng/CSPN">
      <i class="fa fa-code"></i>code</a>
    <!-- Learning affinity to reach fine detailed recovery of depth. No.1 in kitti stereo estimation. -->​
  </li>
  <li>
    <a href="https://arxiv.org/abs/1805.03356">
      SPG-Net: Segmentation Prediction and Guidance Network for Image Inpainting
    </a>
    <br>Yuhang Song, Chao Yang, Yeji Shen, <b>Peng Wang</b>, Qin Huang, C.-C. Jay Kuo
    <br>
    <a href="">BMVC</a>, 2018
<!-- We combined human part segmentation and pose estimation with deep learning to improve both of the tasks over the popular PASCAL Dataset. 
 -->
  </li>
  <li>
    <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w14/Huang_The_ApolloScape_Dataset_CVPR_2018_paper.pdf">
       The ApolloScape Dataset for Autonomous Driving
    </a>
    <br>Xinyu Huang, Xinjing Cheng, Qichuan Geng; Binbin Cao, Dingfu Zhou, <b>Peng Wang</b>, Yuanqing Lin, Yang Ruigang
    <br>
    <a href="http://wad.ai/">CVPR (Workshop of Autonomous Driving)</a>, 2018
    <a href="https://github.com/ApolloScapeAuto/dataset-api">
      <i class="fa fa-code"></i>code</a>, 
    <a href="http://apolloscape.auto/">
      <i class="fa fa-link"></i>project page</a>,
<!-- Next generation of visual based self-driving dataset, millions of densely labelled semantic video, object instance, camera pose, lanemarks, 3D car instances.  Welcome to attend our challenges 
 -->​
  </li>
  <li>
    <a href="https://arxiv.org/abs/1805.04949">
       DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map
    </a>
    <br><b>Peng Wang</b>, Ruigang Yang, Binbin, Cao, Wei Xu, Yuanqing Lin
    <br>
    <a href="">CVPR</a>, 2018
    <a href="https://github.com/pengwangucla/DeLS-3D">
      <i class="fa fa-code"></i>code</a>
  <!-- A large data with dense 3D points, 3D pose tracks and semantic segments is joint release with this paper.
   -->​
  </li>
  <li>
    <a href="https://arxiv.org/abs/1803.05648">
       LEGO: Learning Edge with Geometry all at Once by Watching Videos
    </a>
    <br>Zhenheng Yang, <b>Peng Wang</b>, Yang Wang, Wei Xu, Ramakant Nevatia
    <br>
    <a href="">CVPR</a>, 2018 (<span class="bold-red">spotlight oral</span>)
    <a href="https://github.com/zhenheny/LEGO">
      <i class="fa fa-code"></i>code</a>
<!-- We jointly learning edge, depth and normal in a self-supervised manner using video only. 
 -->​
  </li>
    <li>
    <a href="https://arxiv.org/abs/1804.04213">
       View Extrapolation of Human Body from a Single Image
    </a>
    <br>Hao Zhu, Hao Su, <b>Peng Wang</b>, Ruigang Yang
    <br>
    <a href="">CVPR</a>, 2018 
<!-- View synthesis on human with various shapes, which is much harder than chair or car. We alleviate the difficulties by depth and camera models.
 -->​
  </li>
  <li>
    <a href="https://arxiv.org/abs/1711.05890">
       Occlusion Aware Unsupervised Learning of Optical Flow
    </a>
    <br>Yang Wang, Yi Yang, Zhenheng Yang,  Liang Zhao, <b>Peng Wang</b>, Wei Xu
    <br>
    <a href="">CVPR</a>, 2018 
<!-- Model occlusion in optical flow estimation through discovering overlapping  after warping with flow..
 -->​
  </li>
   <li>
    <a href="https://arxiv.org/abs/1712.04837">
       MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features
    </a>
    <br>Liang-Chieh  Chen, Alexander  Hermans, George  Papandreou, Florian  Schroff, <b>Peng Wang</b>, Hartwig  Adam
    <br>
    <a href="">CVPR</a>, 2018 
  </li>
  <li>
    <a href="https://arxiv.org/abs/1711.03665">
       Unsupervised Geometry Estimation with Edge-aware Depth-Normal Consistency
    </a>
    <br>Zhenheng Yang, <b>Peng Wang</b>, Wei Xu, Liang Zhao, Ramakant Nevatia
    <br>
    <a href="">AAAI</a>, 2018 (<span class="bold-red">Oral</span>)
  </li>
​</ul>

<h2>2017</h2>
<ul>
  <li>
    <a href="https://arxiv.org/abs/1708.03383">
       Joint Multi-Person Pose Estimation and Semantic Part Segmentation in a Single Image
    </a>
    <br>Fangting Xia, <b>Peng Wang</b>, Alan Yuille,
    <br>
    <a href="">CVPR</a>, 2017</a>
    <a href="https://drive.google.com/file/d/0B8Onlv_N6muaLTBMa2d5em1oLW8/view?usp=sharing">
      <i class="fa fa-database"></i>data</a>,
<!-- We combined human part segmentation and pose estimation with deep learning to improve both of the tasks over the popular PASCAL Dataset. 
 -->
  </li>
</ul>
<h2>2016</h2>
<ul>
  <li>
    <a href="https://papers.nips.cc/paper/6502-surge-surface-regularized-geometry-estimation-from-a-single-image">
       SURGE: Surface Regularized Geometric Estimation from a Single Image
    </a>
    <br><b>Peng Wang</b>, Xiaohui Shen, Bryan Russel, Scott Cohen, Brian Price, Alan Yuille
    <br>
    <a href="">NIPS</a>, 2016</a>
    <a href="https://drive.google.com/open?id=0B7DaWBKShuMBcDdTZS0weGxxLUU">
      <i class="fa fa-book"></i>supplimentary</a>,
    <a href="https://www.youtube.com/attribution_link?a=BmYKb8AULoU&u=%2Fwatch%3Fv%3DknvojOOeqMU%26feature%3Dshare">
      <i class="fa fa-play-circle"></i>video</a>,
<!-- We regularize depth and normal prediction by using edge and 3D surface prediction from a single image, which enforces planar equation inside a DCRF layer, and achieves better results visually and quantitatively.
 -->​
  </li>
  <li>
    <a href="https://arxiv.org/abs/1511.06881">
       Zoom Better to See Clearer: Human and Object Part Segmentation with Auto Zoom Net
    </a>
    <br>Fangting Xia, <b>Peng Wang</b>, Liang-Chieh Chen,  Alan Yuille
    <br>
    <a href="">ECCV</a>, 2016</a>
<!-- By inducing a scale estimator for each object and it corresponding parts, we obtain much better results in terms of segmentation accuracy.
 -->
  </li>
  <li>
    <a href="https://arxiv.org/abs/1511.06457">
       DOC: Deep OCclusion Recovering From A Single Image
    </a>
    <br><b>Peng Wang</b>,  Alan Yuille
    <br>
    <a href="">ECCV</a>, 2016</a>
    <a href="https://github.com/pengwangucla/DOC">
    <i class="fa fa-code"></i>code</a>,
    <a href="https://sites.google.com/view/pasd/dataset">
    <i class="fa fa-database"></i>data(pascal in detail)</a>
<!-- Using the deep fully convolutional network for occlusion boundary inference from a single image, which is order of magnitude faster and more accurate than previous works. Also we labelled occlusion relationship over PASCAL 20 objects.
 -->
  </li>
  <li>
    <a href="https://arxiv.org/abs/1508.03881">
       Pose-Guided Human Parsing with Deep Learned Features
    </a>
    <br>Fangting Xia, <b>Peng Wang*</b>, Jun Zhu* ,  Alan Yuille,
    <br>
    <a href="">AAAI</a>, 2016</a> (<span class="bold-red">Oral</span>) (*Equal Contribution)
  </li>
</ul>


<h2>2015</h2>
<ul>
  <li>
    <a href="https://arxiv.org/abs/1505.00276">
       Joint Object and Part Segmentation using Deep Learned Potentials
    </a>
    <br><b>Peng Wang</b>, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian Price, Alan Yuille
    <br>
    <a href="">ICCV</a>, 2015</a> 
    <a href="https://drive.google.com/file/d/0B1STToBvqKE6bE9qQmhiczFtdXM/view">
    <i class="fa fa-book"></i>supplimentary</a>,
    <a href="https://drive.google.com/file/d/0B1STToBvqKE6b09zSHRjMGhFVUk/view">
    <i class="fa fa-database"></i>horse_cow_data</a>,
    <a href="https://drive.google.com/file/d/0B1STToBvqKE6V29ZRGFJRGFpNUE/view">
    <i class="fa fa-database"></i>Pascal_animal_trainval_list</a>,
    <a href="https://competitions.codalab.org/competitions/4551">
    <i class="fa fa-database"></i>Part Challenge</a>
<!-- Part gives more details while object automatically provide long-range context,  a supervised part based segmentation strategy for animals. 
 --> 
  </li>
  <li>
    <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Towards_Unified_Depth_2015_CVPR_paper.pdf">
       Towards Unified Depth and Semantic Prediction from a Single Image
    </a>
    <br><b>Peng Wang</b>, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian Price, Alan Yuille,
    <br>
    <a href="">CVPR</a>, 2015</a> (*Equal Contribution)
    <a href="https://www.youtube.com/watch?v=v6---biFPds&feature=youtu.be">
    <i class="fa fa-play-circle"></i>application video</a>,
    <a href="https://drive.google.com/file/d/0B1STToBvqKE6dXZ2RzY4a3l0ME0/view">
    <i class="fa fa-database"></i>depth results on NYU v2 test</a>
<!-- A fresh new joint learning and inference method for joint segmentation and depth estimation with a unified CNN & HCRF. 
 --> 
  </li>
  <li>
    <a href="https://drive.google.com/file/d/0B7DaWBKShuMBYURoOFd3bmV0eW8/view">
       Learning a Photo Cropping Cascade
    </a>
    <br><b>Peng Wang</b>, Zhe Lin,  Radomir Mech
    <br>
    <a href="">WACV</a>, 2015</a>
    <a href="https://drive.google.com/file/d/0B7DaWBKShuMBbjFnODdvX25Ga2s/view?usp=sharing">
    <i class="fa fa-book"></i>supplimentary</a>,
    <a href="http://fangchen.org/proj_page/FLMS_mm14/data/radomir500_gt/release_data.tar">
    <i class="fa fa-database"></i>dataset (crop label)</a>,
    <a href="http://fangchen.org/proj_page/FLMS_mm14/data/radomir500_image/image.tar">
    <i class="fa fa-database"></i>dataset images</a>
    <a href="https://www.dropbox.com/sh/3kp9hy17haho1lx/AABO0FPr0VPXFuIZ5rXkkr6_a?dl=0">
    <i class="fa fa-database"></i>test images</a>
<!-- Propose photo cropping candidates to increase visual satisfaction of your photos. An efficient cascade branch and bound algorithm for selecting sliding windows that optimizing cascade type energy or models.   Section Best Paper Award. 
 --> 
  </li>
  <li>
    <a href="https://drive.google.com/file/d/0B7DaWBKShuMBWG1UbUsyMVQtSXM/view?usp=sharing">
       Error Factor Analysis for Wild-scene Image Labelling
    </a>
    <br><b>Peng Wang</b>, Alan Yuille
    <br>
    <a href="">WACV</a>, 2015</a> (*Equal Contribution)
    <a href="https://drive.google.com/open?id=0B7DaWBKShuMBSklvQ2xaNTVQd1k">
    <i class="fa fa-code"></i>code with need materials</a>,
<!-- Better way to evaluate your segmentation beyond IOU, know where the error mostly happened.
 -->
  </li>
</ul>

<h2>Before 2015</h2>
<ul>
  <li>
    <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Supervised_Kernel_Descriptors_2013_CVPR_paper.pdf">
       Supervised Kernel Descriptor for Visual Recognition
    </a>
    <br><b>Peng Wang</b>, Jingdong Wang, Gang Zeng, Weiwei Xu, Hongbin Zha, Shipeng Li
    <br>
    <a href="">CVPR</a>, 2013</a>
<!-- A supervised approach for learning low level patch descriptors from image label for image recognition (Early version of deep representation learning,  but with bag of words framework).
 -->​
  </li>
  <li>
    <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/CVPR12-Saliency.pdf">
       Salient Object Detection for Searched Web Images via Global Saliency
    </a>
    <br><b>Peng Wang</b>, Jingdong Wang, Gang Zeng, Jie Feng, Hongbin Zha, Shipeng Li
    <br>
    <a href="">CVPR</a>, 2012</a>
    <a href="https://drive.google.com/open?id=0B7DaWBKShuMBMHF2dXRrdVNxRkE">
    <i class="fa fa-database"></i>web saliency data</a>,
<!-- This is early work using box regression for object detection ( Related with recent direct box regression strategies with Deep Learning for object detection (e.g. Deep Salient Object, Dense box). This method handle web scale data based on random forest regression
 -->
  </li>
  <li>
    <a href="https://drive.google.com/open?id=1eo7EixKGwUlOaFG09s6uYwLCKUsWF03A">
       Contextual Dominant Color Name Extraction for Web Image Search
    </a>
    <br><b>Peng Wang</b>,​ Dongqing Zhang, Gang Zeng, Jingdong Wang
    <br>
    <a href="">ICME (Workshop of Structure-sensitive Superpixels via Geodesic Distance)</a>, 2012</a>
<!-- Extract human perceptually dominant color of images, considering the contextual pixels
 -->​
  </li>
  <li>
    <a href="https://drive.google.com/open?id=1r1m3dlh8S262Tb--hTqb5E_kZkDE0M2P">
       Color Filter for Image Search 
    </a>
    <br><b>Peng Wang</b>, Dongqing Zhang, Jingdong Wang, Zhong Wu, Xian-Sheng Hua, Shipeng Li
    <br>
    <a href="">ACM Multi Media(MM), Demo Abstract</a>, 2012</a>
  </li>
  <li>
    <a href="https://link.springer.com/article/10.1007/s11263-012-0588-6">
       Structure-sensitive Superpixels via  Geodesic Distance
    </a>
    <br><b>Peng Wang</b>, Gang Zeng, Rui Gan, Jingdong Wang, Hongbin Zha
    <br>
    <a href="">IJCV</a>, 2013</a>
  </li>
  <li>
    <a href="https://drive.google.com/open?id=1_Gdcw_MS3E_uQhPtOxE58o4wwRxcU1MI">
       Structure-sensitive Superpixels via  Geodesic Distance
    </a>
    <br>Gang Zeng, <b>Peng Wang</b>, Jingdong Wang, Rui Gan, Hongbin Zha
    <br>
    <a href="">ICCV</a>, 2011</a>
<!-- An image over-segmentation method aware of image structure information. It combines geometric flow and soft clustering
 -->
  </li>
</ul>
</div>


<a name="prod"></a>
<div id="products">
<h1>Products</h1>
    <h3> Delivered to Bytedance Apps, where I lead the effort in algorithm design and development (linked to some demo videos) </h3>
    <li> <a href="https://lv.ulikecam.com/activity/lv/sharevideo?template_id=6963083926369799432"> 3D photo zoom effect </a> </li>
    <li> <a href="https://lv.ulikecam.com/activity/lv/sharevideo?template_id=7055621060586261798"> Cyberpunk photo effect </a> </li>
    <li> <a href="https://www.douyin.com/video/7054732546382351652?previous_page=app_code_link"> AR City Effect </a>
         <a href="https://mp.weixin.qq.com/s/3JMZ1JrEoaTJ5cPMkOwNDg"> <i class="fa fa-book"></i> Public Tech Blog </a> </li>
    <li> <a href="https://vm.tiktok.com/TTPdkXAm8y/"> Virtual Object AR Attachment </a> </li>
    <li> <a href="https://webmaster.tuchong.com/t/72343097/"> SkyAR Effect </a> </li>
</div>


<a name="work"></a>
<div id="working">
<h1>Working Experience</h1>

<ul>
<li> Staff Research Scientist, Bytedance, 2019 - </li>
<li> Senior Research Scientist, Baidu, 2017 - 2019 </li>
<li> Intern, Google Research, Summer 2016 </li>
<li> Intern, Adobe Research, Summer 2014, Spring, 2016 </li>
<li> Research Assistant, UCLA, 2013 - 2017 </li>
<li> Intern, Microsoft Research Asia, Summer - Winter 2012 </li>
</ul>
</div>


<a name="tea"></a> <!-- <div id="line"></div> -->

<div id="teaching">
    <h1>Teaching & Research Activities</h1>
    <ul>
      <li>Chair organizer of  
      <a href="http://apolloscape.auto/ECCV/index.html"> workshop on ApolloScape: 3D Understanding for Autonomous Driving</a>, ECCV 2018
      <li>Staff member of <a href="http://wad.ai">workshop on Autonomous Driving</a>, CVPR 2018</li>
      <li>Staff member of <a href="https://sites.google.com/view/pasd">workshop on PASCAL in details</a>, CVPR 2017</li>
      <li>
       Teaching assistant of 
       <a href="">Stochastic Processes</a> at PKU 
      </li>
      <li> Teaching assistant of <a href="">STAT 261C</a> at UCLA

    </ul>

    <div id="clear"></div>
</div>

</div>
<!-- <div id="design">Designed by <a href="http://www.cs.cmu.edu/~muli/">Mu Li</a>. Last updated <a>07/01/14 </a></div>  -->


</body>
</html>
